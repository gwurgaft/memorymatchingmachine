{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b2b3b6",
   "metadata": {},
   "source": [
    "Two primitives to work with data: torch.utils.data.Dataloader and torch.utils.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd6a1ab",
   "metadata": {},
   "source": [
    "Dataset stores the samples and corresponding labels, Dataloader wraps an iterable around the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577f5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d883970",
   "metadata": {},
   "source": [
    "Offers torchtext, torchvision, and torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb0282a",
   "metadata": {},
   "source": [
    "torchvision.datasets contain Dataset objects for real world vision\n",
    "Transform (modifies the samples), target transform modifies the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974e067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c0277",
   "metadata": {},
   "source": [
    "Pass dataset as argument to dataloader. Wraps iterable over dataset and supports automatic batching, sampling, shuffling and multiprocess data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325c8325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "#Each element in dataloader iterable will return batch of 64 features and labels\n",
    "batch_size=64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "#Look into this more\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195bd1c",
   "metadata": {},
   "source": [
    "To define neural network in pytorch, create class inherting from nn.Module\n",
    "Layers of network in __init__ function, specify how layers pass through the network in the forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ac1fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e743f",
   "metadata": {},
   "source": [
    "Need loss function and optimizer to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9926169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a77c8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd893ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932af50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------------\n",
      "loss: 2.299927 [   64/60000]\n",
      "loss: 2.290554 [ 6464/60000]\n",
      "loss: 2.272920 [12864/60000]\n",
      "loss: 2.269971 [19264/60000]\n",
      "loss: 2.247929 [25664/60000]\n",
      "loss: 2.228224 [32064/60000]\n",
      "loss: 2.236415 [38464/60000]\n",
      "loss: 2.205020 [44864/60000]\n",
      "loss: 2.200093 [51264/60000]\n",
      "loss: 2.169272 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.164590 \n",
      "\n",
      "Epoch 2\n",
      "---------------------------\n",
      "loss: 2.171118 [   64/60000]\n",
      "loss: 2.163300 [ 6464/60000]\n",
      "loss: 2.108594 [12864/60000]\n",
      "loss: 2.125295 [19264/60000]\n",
      "loss: 2.075320 [25664/60000]\n",
      "loss: 2.018136 [32064/60000]\n",
      "loss: 2.045890 [38464/60000]\n",
      "loss: 1.971141 [44864/60000]\n",
      "loss: 1.979437 [51264/60000]\n",
      "loss: 1.906013 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 1.906195 \n",
      "\n",
      "Epoch 3\n",
      "---------------------------\n",
      "loss: 1.934625 [   64/60000]\n",
      "loss: 1.907697 [ 6464/60000]\n",
      "loss: 1.795442 [12864/60000]\n",
      "loss: 1.831597 [19264/60000]\n",
      "loss: 1.731190 [25664/60000]\n",
      "loss: 1.673362 [32064/60000]\n",
      "loss: 1.700155 [38464/60000]\n",
      "loss: 1.603705 [44864/60000]\n",
      "loss: 1.634462 [51264/60000]\n",
      "loss: 1.529153 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.544159 \n",
      "\n",
      "Epoch 4\n",
      "---------------------------\n",
      "loss: 1.607296 [   64/60000]\n",
      "loss: 1.572084 [ 6464/60000]\n",
      "loss: 1.427416 [12864/60000]\n",
      "loss: 1.487774 [19264/60000]\n",
      "loss: 1.382259 [25664/60000]\n",
      "loss: 1.367118 [32064/60000]\n",
      "loss: 1.382686 [38464/60000]\n",
      "loss: 1.310542 [44864/60000]\n",
      "loss: 1.346434 [51264/60000]\n",
      "loss: 1.249413 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.270208 \n",
      "\n",
      "Epoch 5\n",
      "---------------------------\n",
      "loss: 1.344651 [   64/60000]\n",
      "loss: 1.323815 [ 6464/60000]\n",
      "loss: 1.165634 [12864/60000]\n",
      "loss: 1.257680 [19264/60000]\n",
      "loss: 1.145480 [25664/60000]\n",
      "loss: 1.161407 [32064/60000]\n",
      "loss: 1.181637 [38464/60000]\n",
      "loss: 1.123959 [44864/60000]\n",
      "loss: 1.162486 [51264/60000]\n",
      "loss: 1.081361 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.097766 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3fef864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Pytorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved Pytorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb8382dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c4e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d520b19",
   "metadata": {},
   "source": [
    "Tensors are specialized data structures similar to arrays and matrices. Use tensors to encode inputs and outputs of a model and the model's parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd9eda",
   "metadata": {},
   "source": [
    "Can run on GPUs or other hardware accelerators, share underlying memory with NumPy. Can optimize for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "041fc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b2a413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing tensor\n",
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da87f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can be initialized from numpy arrays\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6773ba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.5021, 0.9237],\n",
      "        [0.1045, 0.4445]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#retains properties of argument tensor unless overridden\n",
    "x_ones = torch.ones_like(x_data)\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75e28e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.6833, 0.1776, 0.1089],\n",
      "        [0.5026, 0.8643, 0.4436]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Shape is a tuple of tensor dimensions, dimensionality of the output tensor\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37f255f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "#attributes: shape, datatype, and device they are stored\n",
    "tensor = torch.rand(3,4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8acfd1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#standard numpy like:\n",
    "tensor = torch.ones(4,4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:,0]}\")\n",
    "print(f\"Last column: {tensor[:,-1]}\")\n",
    "tensor[:,1]=0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ee00ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#Joining tensors through cat\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43869317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#arithmetic operations\n",
    "#matrix multiplication\n",
    "y1 = tensor @ tensor.T\n",
    "print(y1)\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "#computes element-wise product\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23f167fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n",
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "#Single element tensors: aggregating all values of a tensor into one value\n",
    "agg = tensor.sum()\n",
    "print(agg)\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "315dd2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "#in-place operand\n",
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36cd7a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#tensor to numpy\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7dcc911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "#changes reflect in numpy array\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "098a0340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#numpy to tensor\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "print(f\"n: {n}\")\n",
    "print(f\"t: {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f86f22e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: [2. 2. 2. 2. 2.]\n",
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"n: {n}\")\n",
    "print(f\"t: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b5d1c",
   "metadata": {},
   "source": [
    "Data samples messy, want our dataset decoupled from our model training code. DataLoader (wraps an iterable around dataset for easy access to samples) and Dataset (stores samples and corresponding labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31f33305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", #path where data is stored\n",
    "    train = True, #training or test dataset\n",
    "    download = True, #downloads data from internet if not available at root\n",
    "    transform = ToTensor() #feature and label transformations\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "945726e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Iterating and Visualizing Dataset\n",
    "labels_map = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdce1c2",
   "metadata": {},
   "source": [
    "Custom dataset implements init, len and, getitem. FashionMNIST images are stored in directory img_dir and labels are stored separately in a CSV file annotations_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c09598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    #instantiating the dataset object\n",
    "    #\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    #returns number of samples in our dataset\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    #loads and returns a sample from a dataset at given index\n",
    "    #identifies location on disk, converts to tensor (decode image), and retrieves the label\n",
    "    #Calls transform funcs and returns tensor image and corresponding label\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = decode_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b23744",
   "metadata": {},
   "source": [
    "Preparing your data for training with DataLoaders.\n",
    "Dataloader is an iterable, can do minibatches, and reshuffles the data at every epoch to reduce model overfitting and use multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fda5f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb96a8",
   "metadata": {},
   "source": [
    "Can iterate through dataset as needed. Each iteration returns a batch of train_features and train_labels (containing batch_size 64).\n",
    "After we iterate over all batches the data is shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "922c0edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featrue batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "#Display image and label\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Featrue batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c66602",
   "metadata": {},
   "source": [
    "Transforms perform some data manipulation.\n",
    "Transform modifies the features, target_transform modifies the labels\n",
    "Features are in PIL image format, labels are integers\n",
    "Features need to be normalized tensors and labels need to be one-hot encoded tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2038b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713745d4",
   "metadata": {},
   "source": [
    "ToTensor converts PIL image or NumPy array to float tensor and scales image's pixel values in range [0,1].\n",
    "Lambda transforms apply user-defined lambda function. \n",
    "Function turns the integer to a one hot encoded tensor (zero tensor of size 10 -- # of labels, then assigns a value of 1 on the index as given bu label y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c91dd",
   "metadata": {},
   "source": [
    "Building the Neural Network: layers/modules that perform operations on data. Torch.nn provides building blocks to build your own neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4906963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e6d4ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb22f8",
   "metadata": {},
   "source": [
    "Subclass nn.Modue, initialize neural network, implements operations on input data in the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e3cfb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d7b7500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb71c7",
   "metadata": {},
   "source": [
    "Pass the input data, executes model's forward along with background operations. Returns 2-dim tensor of 10 raw predicted values for each class and individual values of each output. nn.Softmax gets the prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcf368e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "# print(pred_probab)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07785a81",
   "metadata": {},
   "source": [
    "nn.flatten layer converts each 2D 28x28 image to a contiguous array of 784 pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee2b0710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "333a5409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1499, 0.5128, 0.3023,  ..., 0.7438, 0.5144, 0.0647],\n",
      "         [0.5940, 0.9664, 0.5005,  ..., 0.4283, 0.0769, 0.9604],\n",
      "         [0.4170, 0.2685, 0.9758,  ..., 0.3078, 0.4443, 0.8328],\n",
      "         ...,\n",
      "         [0.9465, 0.8249, 0.6175,  ..., 0.8987, 0.8608, 0.1648],\n",
      "         [0.2552, 0.9921, 0.6825,  ..., 0.2603, 0.0881, 0.9165],\n",
      "         [0.0665, 0.3422, 0.7365,  ..., 0.9755, 0.1568, 0.5573]],\n",
      "\n",
      "        [[0.5317, 0.6538, 0.7180,  ..., 0.3585, 0.6543, 0.3792],\n",
      "         [0.9606, 0.6672, 0.4040,  ..., 0.3308, 0.0029, 0.7779],\n",
      "         [0.6311, 0.1739, 0.1552,  ..., 0.0173, 0.9917, 0.9019],\n",
      "         ...,\n",
      "         [0.8427, 0.6045, 0.4019,  ..., 0.4433, 0.2986, 0.9957],\n",
      "         [0.3963, 0.3576, 0.0623,  ..., 0.2409, 0.5728, 0.7739],\n",
      "         [0.1775, 0.9295, 0.2069,  ..., 0.2185, 0.1904, 0.5429]],\n",
      "\n",
      "        [[0.3921, 0.8695, 0.6725,  ..., 0.1988, 0.6637, 0.9307],\n",
      "         [0.5682, 0.0694, 0.8414,  ..., 0.1541, 0.1740, 0.1050],\n",
      "         [0.4729, 0.0505, 0.0105,  ..., 0.9993, 0.9126, 0.2568],\n",
      "         ...,\n",
      "         [0.5171, 0.1620, 0.9856,  ..., 0.3122, 0.6913, 0.9954],\n",
      "         [0.2255, 0.5540, 0.6325,  ..., 0.0610, 0.2620, 0.2587],\n",
      "         [0.3478, 0.0207, 0.5743,  ..., 0.9744, 0.9930, 0.0824]]])\n",
      "tensor([[0.1499, 0.5128, 0.3023,  ..., 0.9755, 0.1568, 0.5573],\n",
      "        [0.5317, 0.6538, 0.7180,  ..., 0.2185, 0.1904, 0.5429],\n",
      "        [0.3921, 0.8695, 0.6725,  ..., 0.9744, 0.9930, 0.0824]])\n"
     ]
    }
   ],
   "source": [
    "print(input_image)\n",
    "print(flat_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ff712",
   "metadata": {},
   "source": [
    "linear layer applies a linear transformation on input using stored weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aab8a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91849b",
   "metadata": {},
   "source": [
    "Non-linear activations create complex mappings between model's inputs and outputs. Introduce nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0a147cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.2357,  0.4367, -0.4475,  0.1966, -0.0390, -0.0804, -0.0255, -0.2463,\n",
      "          0.1718, -0.1379, -0.9255, -0.3986, -0.0055,  0.3634, -0.0408, -0.3125,\n",
      "         -0.0836,  0.2721,  0.1517, -0.4176],\n",
      "        [-0.1885,  0.4156, -0.3293,  0.0895,  0.1444, -0.1808,  0.0825, -0.2913,\n",
      "         -0.0957,  0.0867, -0.7684, -0.5227,  0.2723,  0.2543,  0.0174, -0.0996,\n",
      "         -0.0882,  0.2158,  0.0828, -0.6404],\n",
      "        [-0.4000,  0.3350, -0.5802, -0.0096,  0.1796, -0.0468, -0.2470, -0.4162,\n",
      "          0.1221, -0.1807, -1.1182, -0.2311, -0.0764,  0.4609,  0.0201, -0.0798,\n",
      "         -0.0342,  0.6605,  0.1112, -0.5658]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.4367, 0.0000, 0.1966, 0.0000, 0.0000, 0.0000, 0.0000, 0.1718,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.3634, 0.0000, 0.0000, 0.0000, 0.2721,\n",
      "         0.1517, 0.0000],\n",
      "        [0.0000, 0.4156, 0.0000, 0.0895, 0.1444, 0.0000, 0.0825, 0.0000, 0.0000,\n",
      "         0.0867, 0.0000, 0.0000, 0.2723, 0.2543, 0.0174, 0.0000, 0.0000, 0.2158,\n",
      "         0.0828, 0.0000],\n",
      "        [0.0000, 0.3350, 0.0000, 0.0000, 0.1796, 0.0000, 0.0000, 0.0000, 0.1221,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.4609, 0.0201, 0.0000, 0.0000, 0.6605,\n",
      "         0.1112, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430a116",
   "metadata": {},
   "source": [
    "nn.Sequential os ordered container of modules, data passed through all modules in same order as defined (put together a quick network like seq_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85731ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3472, -0.1560,  0.0596, -0.2420,  0.0865, -0.3657, -0.0122, -0.1738,\n",
      "          0.0249, -0.2875],\n",
      "        [-0.1635, -0.1849,  0.0839, -0.1666,  0.1703, -0.2322, -0.0559, -0.1043,\n",
      "         -0.0092, -0.1570],\n",
      "        [-0.1896, -0.1388,  0.0671, -0.0317,  0.2700, -0.1053, -0.0228, -0.2569,\n",
      "          0.0957, -0.3069]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec9d62",
   "metadata": {},
   "source": [
    "Logits (raw values) passed to the nn.Softmax module, scaled to values [0,1]. represents the model's predicted probabilities for each class. Dim indicates which the values must sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "026d4a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0803, 0.0973, 0.1207, 0.0892, 0.1240, 0.0789, 0.1123, 0.0956, 0.1165,\n",
      "         0.0853],\n",
      "        [0.0915, 0.0895, 0.1171, 0.0912, 0.1277, 0.0854, 0.1018, 0.0970, 0.1067,\n",
      "         0.0921],\n",
      "        [0.0868, 0.0913, 0.1122, 0.1016, 0.1374, 0.0944, 0.1025, 0.0811, 0.1154,\n",
      "         0.0772]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(pred_probab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "390a6d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([0.0922, 0.0911, 0.1162, 0.0733, 0.1132, 0.1111, 0.1089, 0.0969, 0.0913,\n",
    "         0.1058])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f1f0a0",
   "metadata": {},
   "source": [
    "Many layers inside the neural network are parametrized (have associated weights and biases optimized during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5f5f6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layers: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[ 0.0227, -0.0093,  0.0293,  ..., -0.0053,  0.0130, -0.0305],\n",
      "        [ 0.0189, -0.0161, -0.0065,  ..., -0.0184,  0.0294,  0.0051]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layers: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values: tensor([-0.0335, -0.0162], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layers: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[ 0.0324,  0.0430, -0.0282,  ...,  0.0419,  0.0081,  0.0115],\n",
      "        [-0.0016, -0.0421,  0.0084,  ...,  0.0375,  0.0352, -0.0406]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layers: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values: tensor([ 0.0415, -0.0242], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layers: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values: tensor([[ 0.0256,  0.0267, -0.0396,  ..., -0.0038,  0.0424,  0.0014],\n",
      "        [ 0.0184, -0.0433,  0.0155,  ...,  0.0365,  0.0273, -0.0368]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layers: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values: tensor([-0.0102, -0.0407], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layers: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66124c",
   "metadata": {},
   "source": [
    "Automatic differentiation with torch.autograd. When training neural networks, the most frequently used algorithm is back propagation. parameters (model weights) are adjusted according to the gradient of the loss function wrt a given parameter.\n",
    "Torch.autograd is a built-in differentiation engine, supports automatic computation of gradient for any computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "892ad73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb786ca",
   "metadata": {},
   "source": [
    "w anb b are parameters we need to optimize, need to compute the gradients of the loss function wrt those variables (requires_grad property)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78289ff8",
   "metadata": {},
   "source": [
    "Function we apply to tensors to construct computational graph (object of class Function). Knows how to compute the function in the forward direction and how to compute its derivative during the backward propagation step (grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33784203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f9d61f0f5e0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f9d61f0f520>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdec23e",
   "metadata": {},
   "source": [
    "Computing gradients: optimize weights of parameters in the neural network, compute derivatives of our loss function wrt parameters (w and b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d87842fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3149, 0.1706, 0.2649],\n",
      "        [0.3149, 0.1706, 0.2649],\n",
      "        [0.3149, 0.1706, 0.2649],\n",
      "        [0.3149, 0.1706, 0.2649],\n",
      "        [0.3149, 0.1706, 0.2649]])\n",
      "tensor([0.3149, 0.1706, 0.2649])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63326540",
   "metadata": {},
   "source": [
    "Disable Gradient tracking -- all tensors with requires_grad=True track their computational history and support gradient computation. Some cases when we do not need to do that, trained model and want to apply it with input data (only forward computations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29492800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0106b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057cf092",
   "metadata": {},
   "source": [
    "Disable gradient tracking to mark some parameters as frozen parameters, speed up computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba261b16",
   "metadata": {},
   "source": [
    "Autograds keep a record of data (tensors) and all executed operations in directed acyclic graph consisting of Function objects.\n",
    "Automatically compte the gradients using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49368921",
   "metadata": {},
   "source": [
    "Forward pass: run the requested operation to compute a resulting tensor, maintain the operation's gradient function in DAG\n",
    "Backward pass (.backward()): computes the gradients .grad_fn, accumulates them in the respective tensor's .grad attribute, propagates all the way to the leaf tensors using chaine rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a7949",
   "metadata": {},
   "source": [
    "In some cases when output is a tensor, it allows you to compute a Jacobian product, not the actual gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45bf48bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]], requires_grad=True)\n",
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "print(inp)\n",
    "out = (inp+1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633a51e",
   "metadata": {},
   "source": [
    "Optimizing model parameter: train, validate and test our model by optimizing its parameters on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b7c2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce449a",
   "metadata": {},
   "source": [
    "Number of epochs: number of times to iterate over the dataset,\n",
    "Batch size: number of data samples propagated through the network before the parameters are updated,\n",
    "Learning rate: how much to update models parameters at each batch/epoch, smaller valies yield low learning speed, large values result in unpredictable behavior during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d282d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af964c34",
   "metadata": {},
   "source": [
    "train and optimize our model, each iteration of the optimization loop is called an epoch.\n",
    "Train loop: iterate over the training dataset, try to converge optimal parameters\n",
    "validation/test loop: iterate over the test dataset to check if model performance is improving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8354b85",
   "metadata": {},
   "source": [
    "Loss function: training data, untrained network not likely to give the correct answer, loss function measures the degree of dissimilarity of obtained result to the target value (loss function we want to minimize during training),\n",
    "Make a prediction using the inputs of our given data sample and compare it against the true data label value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d064423a",
   "metadata": {},
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7476bd2e",
   "metadata": {},
   "source": [
    "Optimization adjusts model parameters to reduce model error, initialize the optimizer by registering the model's parameters that need to be trained, and passing in the learning rate hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7e635f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5680e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    #Set model to training mode  important for batch normalization and dropout layers\n",
    "    #Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    #Set the model to evaluation mode  important for batch normalization and dropout layers\n",
    "    #Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e997a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------------\n",
      "loss: 2.295537 [   64/60000]\n",
      "loss: 2.288702 [ 6464/60000]\n",
      "loss: 2.271168 [12864/60000]\n",
      "loss: 2.271898 [19264/60000]\n",
      "loss: 2.243320 [25664/60000]\n",
      "loss: 2.234387 [32064/60000]\n",
      "loss: 2.230345 [38464/60000]\n",
      "loss: 2.195346 [44864/60000]\n",
      "loss: 2.197858 [51264/60000]\n",
      "loss: 2.182171 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 2.163123\n",
      "\n",
      "Epoch 2\n",
      "---------------------------\n",
      "loss: 2.170091 [   64/60000]\n",
      "loss: 2.160707 [ 6464/60000]\n",
      "loss: 2.103638 [12864/60000]\n",
      "loss: 2.129666 [19264/60000]\n",
      "loss: 2.064082 [25664/60000]\n",
      "loss: 2.025938 [32064/60000]\n",
      "loss: 2.044333 [38464/60000]\n",
      "loss: 1.959679 [44864/60000]\n",
      "loss: 1.978667 [51264/60000]\n",
      "loss: 1.919747 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 1.902406\n",
      "\n",
      "Epoch 3\n",
      "---------------------------\n",
      "loss: 1.934423 [   64/60000]\n",
      "loss: 1.905419 [ 6464/60000]\n",
      "loss: 1.781344 [12864/60000]\n",
      "loss: 1.835125 [19264/60000]\n",
      "loss: 1.715710 [25664/60000]\n",
      "loss: 1.675881 [32064/60000]\n",
      "loss: 1.698755 [38464/60000]\n",
      "loss: 1.583272 [44864/60000]\n",
      "loss: 1.620302 [51264/60000]\n",
      "loss: 1.526225 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 1.533610\n",
      "\n",
      "Epoch 4\n",
      "---------------------------\n",
      "loss: 1.598863 [   64/60000]\n",
      "loss: 1.568752 [ 6464/60000]\n",
      "loss: 1.408711 [12864/60000]\n",
      "loss: 1.491341 [19264/60000]\n",
      "loss: 1.366907 [25664/60000]\n",
      "loss: 1.365755 [32064/60000]\n",
      "loss: 1.380355 [38464/60000]\n",
      "loss: 1.285321 [44864/60000]\n",
      "loss: 1.329820 [51264/60000]\n",
      "loss: 1.237957 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.262033\n",
      "\n",
      "Epoch 5\n",
      "---------------------------\n",
      "loss: 1.336736 [   64/60000]\n",
      "loss: 1.326659 [ 6464/60000]\n",
      "loss: 1.151758 [12864/60000]\n",
      "loss: 1.266347 [19264/60000]\n",
      "loss: 1.138945 [25664/60000]\n",
      "loss: 1.164848 [32064/60000]\n",
      "loss: 1.185290 [38464/60000]\n",
      "loss: 1.102137 [44864/60000]\n",
      "loss: 1.151370 [51264/60000]\n",
      "loss: 1.070822 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.094845\n",
      "\n",
      "Epoch 6\n",
      "---------------------------\n",
      "loss: 1.163172 [   64/60000]\n",
      "loss: 1.173319 [ 6464/60000]\n",
      "loss: 0.983919 [12864/60000]\n",
      "loss: 1.126862 [19264/60000]\n",
      "loss: 0.999115 [25664/60000]\n",
      "loss: 1.032288 [32064/60000]\n",
      "loss: 1.066178 [38464/60000]\n",
      "loss: 0.986636 [44864/60000]\n",
      "loss: 1.036631 [51264/60000]\n",
      "loss: 0.966960 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.987515\n",
      "\n",
      "Epoch 7\n",
      "---------------------------\n",
      "loss: 1.043149 [   64/60000]\n",
      "loss: 1.073560 [ 6464/60000]\n",
      "loss: 0.869309 [12864/60000]\n",
      "loss: 1.033298 [19264/60000]\n",
      "loss: 0.911134 [25664/60000]\n",
      "loss: 0.939409 [32064/60000]\n",
      "loss: 0.988946 [38464/60000]\n",
      "loss: 0.911388 [44864/60000]\n",
      "loss: 0.957586 [51264/60000]\n",
      "loss: 0.897536 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.914143\n",
      "\n",
      "Epoch 8\n",
      "---------------------------\n",
      "loss: 0.955177 [   64/60000]\n",
      "loss: 1.003842 [ 6464/60000]\n",
      "loss: 0.787240 [12864/60000]\n",
      "loss: 0.966023 [19264/60000]\n",
      "loss: 0.852293 [25664/60000]\n",
      "loss: 0.871108 [32064/60000]\n",
      "loss: 0.934802 [38464/60000]\n",
      "loss: 0.860648 [44864/60000]\n",
      "loss: 0.900445 [51264/60000]\n",
      "loss: 0.847158 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.861051\n",
      "\n",
      "Epoch 9\n",
      "---------------------------\n",
      "loss: 0.887228 [   64/60000]\n",
      "loss: 0.951406 [ 6464/60000]\n",
      "loss: 0.726005 [12864/60000]\n",
      "loss: 0.914945 [19264/60000]\n",
      "loss: 0.810283 [25664/60000]\n",
      "loss: 0.819294 [32064/60000]\n",
      "loss: 0.893926 [38464/60000]\n",
      "loss: 0.825062 [44864/60000]\n",
      "loss: 0.857289 [51264/60000]\n",
      "loss: 0.808587 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.820658\n",
      "\n",
      "Epoch 10\n",
      "---------------------------\n",
      "loss: 0.832753 [   64/60000]\n",
      "loss: 0.909668 [ 6464/60000]\n",
      "loss: 0.678289 [12864/60000]\n",
      "loss: 0.874801 [19264/60000]\n",
      "loss: 0.778586 [25664/60000]\n",
      "loss: 0.779424 [32064/60000]\n",
      "loss: 0.860749 [38464/60000]\n",
      "loss: 0.798856 [44864/60000]\n",
      "loss: 0.823670 [51264/60000]\n",
      "loss: 0.777558 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.788546\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce6ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb0d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
